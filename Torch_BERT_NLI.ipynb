{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Torch-BERT-NLI.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwmoQPBr9DNK",
        "outputId": "9aa606b5-86e2-43bf-ef12-645a234c6ea8"
      },
      "source": [
        "import pip\n",
        "for package in [\"sentencepiece\", \"transformers\", \"torch\"]:\n",
        "  try:\n",
        "    __import__('imp').find_module(package)\n",
        "    print(f\"Found {package}\")\n",
        "  except ImportError:\n",
        "    pip.main(['install', package]) \n",
        "\n",
        "import requests\n",
        "from zipfile import ZipFile\n",
        "from io import BytesIO\n",
        "from typing import *\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from os import listdir, path\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, TensorDataset, DataLoader, SequentialSampler, RandomSampler, random_split\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found sentencepiece\n",
            "Found transformers\n",
            "Found torch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-k24nhx-F7H",
        "outputId": "b5c15d04-63d3-4fa7-a638-8824d32abf7b"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eslFTYX69xuD"
      },
      "source": [
        "class DatasetFromZip(object):\n",
        "    def __init__(self, url: str):\n",
        "        self.url = url\n",
        "    \n",
        "    def extract(self, output_path: str = \"\"):\n",
        "        self.output_path = output_path\n",
        "        r = requests.get(self.url)\n",
        "        zipped = ZipFile(BytesIO(r.content))\n",
        "        zipped.extractall(output_path)\n",
        "    \n",
        "    def load_as_pandas(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def generate_data(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "class MNLIDataset(DatasetFromZip):\n",
        "    def __init__(self):\n",
        "        super().__init__(url = \"https://cims.nyu.edu/~sbowman/multinli/multinli_1.0.zip\")\n",
        "        self.class_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
        "        self.class_map = dict([idx for idx in enumerate(self.class_names)]) #idx to name\n",
        "        self.class_map_inv = {v: k for k, v in self.class_map.items()} #name to idx\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "        self.cls_token = [self.tokenizer.cls_token_id]\n",
        "        self.sep_token = [self.tokenizer.sep_token_id]\n",
        "\n",
        "    def extract(self, output_path: str = \"\"):\n",
        "        self.data_files = listdir(output_path) if output_path else listdir()\n",
        "\n",
        "        if \"multinli_1.0\" not in self.data_files:\n",
        "            super().extract(output_path)\n",
        "        elif len(listdir(\"multinli_1.0\")) < 9:\n",
        "            super().extract(output_path)\n",
        "\n",
        "        self.data_path = path.join(output_path, \"multinli_1.0\")\n",
        "        self.data_files = listdir(self.data_path)\n",
        "        self.train_path = path.join(self.data_path, \"multinli_1.0_train.txt\")\n",
        "\n",
        "    def load_as_pandas(self, train_data_only: bool = False) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Note: Loads Entire Dataset in memory. Uses about 1.5GB of RAM\n",
        "        \"\"\"\n",
        "        with open(self.train_path, \"r\") as f:\n",
        "            df = pd.DataFrame([row.split(\"\\t\") for row in f.read().split(\"\\n\")])\n",
        "        df = df.rename(columns=df.iloc[0]).drop(df.index[0])\n",
        "        df = df[~df.sentence1.isna() | ~df.sentence2.isna()]\n",
        "        df = df[df.gold_label != \"\"].reset_index(drop=True)\n",
        "        if train_data_only:\n",
        "            return df[[\"gold_label\", \"sentence1\", \"sentence2\"]]\n",
        "        return df\n",
        "\n",
        "    def __bert_encode(self, text: str, special_tokens: bool = False) -> list:\n",
        "        return self.tokenizer.encode(text, add_special_tokens = special_tokens)\n",
        "        \n",
        "    def __pad(self, seq: list) -> torch.Tensor:\n",
        "        return pad_sequence(seq, batch_first=True)\n",
        "\n",
        "    def generate_data(self, val_split_perc: float, batch_size: int):\n",
        "        \"\"\"\n",
        "        Generates both Train and Val Datasets via torch Dataloader\n",
        "        \"\"\"\n",
        "        token_ids, mask_ids, seg_ids, y = [], [], [], []\n",
        "\n",
        "        with open(self.train_path, \"r\") as f:\n",
        "            data = [row.split(\"\\t\") for row in f.read().split(\"\\n\")]\n",
        "        for idx, row in enumerate(data):\n",
        "            if idx == 0 or idx == len(data) - 1: continue\n",
        "            label, sent1, sent2 = row[0], row[5], row[6]\n",
        "            if label not in self.class_names: continue\n",
        "\n",
        "            sent1, sent2 = self.__bert_encode(sent1), self.__bert_encode(sent2)\n",
        "\n",
        "            pair = self.cls_token + sent1 + self.sep_token + sent2 + self.sep_token\n",
        "            premise_len, hypoth_len = len(sent1), len(sent2)\n",
        "            segment_id = torch.tensor([0] * (premise_len + 2) + [1] * (hypoth_len + 1))\n",
        "            attention_mask_id = torch.tensor([1] * (premise_len + hypoth_len + 3))\n",
        "\n",
        "            token_ids.append(torch.tensor(pair))\n",
        "            seg_ids.append(segment_id)\n",
        "            mask_ids.append(attention_mask_id)\n",
        "            y.append(self.class_map_inv[label])\n",
        "\n",
        "        dataset = TensorDataset(self.__pad(token_ids), self.__pad(mask_ids), self.__pad(seg_ids), torch.tensor(y))\n",
        "        datalen = len(dataset)\n",
        "        val_num = int(datalen * val_split_perc)\n",
        "        train_num = datalen - val_num\n",
        "        train_data, val_data = random_split(dataset, [train_num, val_num])\n",
        "\n",
        "        self.train_data = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "        self.val_data = DataLoader(val_data, shuffle=True, batch_size=batch_size)\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXjG-n2uFu4w"
      },
      "source": [
        "SPLIT_PERC = 0.2\n",
        "BATCH_SIZE = 16 #can't go higher because of GPU memory limits\n",
        "\n",
        "builder = MNLIDataset()\n",
        "builder.extract()\n",
        "builder.generate_data(val_split_perc = SPLIT_PERC, batch_size=BATCH_SIZE)\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBcWeb1LVws4"
      },
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3)\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVGc9yuNV11v",
        "outputId": "dbd46342-30f9-4e19-e95e-b73a165ff305"
      },
      "source": [
        "def multi_acc(y_pred, y_test):\n",
        "  acc = (torch.log_softmax(y_pred, dim=1).argmax(dim=1) == y_test).sum().float() / float(y_test.size(0))\n",
        "  return acc\n",
        "\n",
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.0}\n",
        "]\n",
        "\n",
        "# This variable contains all of the hyperparemeter information our training loop needs\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5, correct_bias=False)\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 109,484,547 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyNXotGiWt09"
      },
      "source": [
        "import time\n",
        "\n",
        "\n",
        "def train(model, train_loader, val_loader, optimizer, EPOCHS):  \n",
        "  total_step = len(train_loader)\n",
        "\n",
        "  for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "    total_train_acc  = 0\n",
        "    for batch_idx, (pair_token_ids, mask_ids, seg_ids, y) in enumerate(train_loader):\n",
        "      optimizer.zero_grad()\n",
        "      pair_token_ids = pair_token_ids.to(device)\n",
        "      mask_ids = mask_ids.to(device)\n",
        "      seg_ids = seg_ids.to(device)\n",
        "      labels = y.to(device)\n",
        "      \n",
        "      loss, prediction = model(pair_token_ids, \n",
        "                             token_type_ids=seg_ids, \n",
        "                             attention_mask=mask_ids, \n",
        "                             labels=labels).values()\n",
        "\n",
        "      # loss = criterion(prediction, labels)\n",
        "      acc = multi_acc(prediction, labels)\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      \n",
        "      total_train_loss += loss.item()\n",
        "      total_train_acc  += acc.item()\n",
        "\n",
        "    train_acc  = total_train_acc/len(train_loader)\n",
        "    train_loss = total_train_loss/len(train_loader)\n",
        "    model.eval()\n",
        "    total_val_acc  = 0\n",
        "    total_val_loss = 0\n",
        "    with torch.no_grad():\n",
        "      for batch_idx, (pair_token_ids, mask_ids, seg_ids, y) in enumerate(val_loader):\n",
        "        optimizer.zero_grad()\n",
        "        pair_token_ids = pair_token_ids.to(device)\n",
        "        mask_ids = mask_ids.to(device)\n",
        "        seg_ids = seg_ids.to(device)\n",
        "        labels = y.to(device)\n",
        "\n",
        "        # prediction = model(pair_token_ids, mask_ids, seg_ids)\n",
        "        loss, prediction = model(pair_token_ids, \n",
        "                             token_type_ids=seg_ids, \n",
        "                             attention_mask=mask_ids, \n",
        "                             labels=labels).values()\n",
        "        \n",
        "        acc = multi_acc(prediction, labels)\n",
        "\n",
        "        total_val_loss += loss.item()\n",
        "        total_val_acc  += acc.item()\n",
        "\n",
        "    val_acc  = total_val_acc/len(val_loader)\n",
        "    val_loss = total_val_loss/len(val_loader)\n",
        "    end = time.time()\n",
        "    hours, rem = divmod(end-start, 3600)\n",
        "    minutes, seconds = divmod(rem, 60)\n",
        "\n",
        "    print(f'Epoch {epoch+1}: train_loss: {train_loss:.4f} train_acc: {train_acc:.4f} | val_loss: {val_loss:.4f} val_acc: {val_acc:.4f}')\n",
        "    print(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))\n",
        "\n",
        "EPOCHS = 5\n",
        "train(model, builder.train_data, builder.val_data, optimizer, EPOCHS)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}